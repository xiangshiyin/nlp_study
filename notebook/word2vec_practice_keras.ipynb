{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: get the online data (based on tensorflow tutorial on word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "['text8']\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from tempfile import gettempdir\n",
    "import collections\n",
    "\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                       local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + local_filename +\n",
    "                      '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        print(f.namelist())\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    ## only consider top n_words in the file\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1)) # get top 50k words in frequency\n",
    "    dictionary = {}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # index those top words by frequency ranking\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index) # collect indices of all words\n",
    "    count[0][1] = unk_count # update the UNK count, which is everything else except top 50k-1\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "#### pull the online data, and transform it\n",
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: constants and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80,  8, 22, 74,  7, 70, 25,  1, 95, 51, 82, 45, 67,  5, 40, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 200000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "valid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: the skip-gram function in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2575, 8045], [168, 501], [28, 11], [5753, 3438], [4135, 6277], [1903, 707], [174, 303], [7110, 1036], [4670, 5610], [6255, 1792]] [0, 0, 1, 0, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30031868, 30031868)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_target),len(word_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17005207, 10000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(count), len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30031868, 15015934)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15000290/30000580 # numbers of postive and negative samples are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4: create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 11:00:02.348072 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 11:00:02.377457 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0717 11:00:02.387816 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5: finish the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### based on old keras modules\n",
    "# # setup a cosine similarity operation which will be output in a secondary model\n",
    "# similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "\n",
    "# # now perform the dot product operation to get a similarity measure\n",
    "# dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "# dot_product = Reshape((1,))(dot_product)\n",
    "# # add the sigmoid output layer\n",
    "# output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# # create the primary training model\n",
    "# model = Model(input=[input_target, input_context], output=output)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# # create a secondary validation model to run our similarity checks during training\n",
    "# validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "W0717 14:46:34.783025 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0717 14:46:34.798624 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0717 14:46:34.802967 4713334208 deprecation.py:323] From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([target, context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = keras.layers.Dot(axes=1, normalize=False)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 6: create the similarity callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1] ## get the indices of words with maximum similarities\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 15:25:55.007726 4713334208 deprecation_wrapper.py:119] From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.7009079456329346\n",
      "Nearest to over: significance, sultan, subdivided, mazda, and, editor, direction, style,\n",
      "Nearest to zero: treatments, murray, fungi, coverage, schemes, frontier, escaped, ceo,\n",
      "Nearest to six: hymn, scenes, recordings, frankfurt, comoros, honda, episodes, allowed,\n",
      "Nearest to would: transmit, sanctuary, lisa, weaker, hub, volumes, pedro, ancestry,\n",
      "Nearest to to: squad, bo, instruments, subsidiary, programmes, sovereign, manpower, homeland,\n",
      "Nearest to than: genesis, afc, arabic, hebrew, rebuilt, guerrilla, seemed, cold,\n",
      "Nearest to on: died, attempt, pioneers, carol, strand, commodore, shuttle, intention,\n",
      "Nearest to the: apache, radiation, byte, agricultural, heat, drunk, fourth, managing,\n",
      "Nearest to so: nuclei, collapsed, canterbury, perry, bart, discovering, release, census,\n",
      "Nearest to more: wto, india, impressed, legal, boom, landscape, hull, russians,\n",
      "Nearest to states: barnes, drinking, thinking, croatian, generic, black, cancer, descriptive,\n",
      "Nearest to its: thou, private, debated, participated, mainland, southwest, inscriptions, honorary,\n",
      "Nearest to only: fully, eve, panels, hate, corresponds, eat, instrument, joseph,\n",
      "Nearest to in: gram, russia, profits, bc, operated, walter, lock, immortal,\n",
      "Nearest to were: alter, periodic, susceptible, expertise, spellings, critics, feast, get,\n",
      "Nearest to his: kai, victorian, acknowledged, rings, april, progress, hormones, hymn,\n",
      "Iteration 100, loss=0.7099900841712952\n",
      "Iteration 200, loss=0.6913892030715942\n",
      "Iteration 300, loss=0.6910303235054016\n",
      "Iteration 400, loss=0.6935179829597473\n",
      "Iteration 500, loss=0.7019034028053284\n",
      "Iteration 600, loss=0.6950517892837524\n",
      "Iteration 700, loss=0.6880127787590027\n",
      "Iteration 800, loss=0.6633584499359131\n",
      "Iteration 900, loss=0.6945581436157227\n",
      "Iteration 1000, loss=0.7181136012077332\n",
      "Iteration 1100, loss=0.7071476578712463\n",
      "Iteration 1200, loss=0.6854698657989502\n",
      "Iteration 1300, loss=0.6961133480072021\n",
      "Iteration 1400, loss=0.6794294714927673\n",
      "Iteration 1500, loss=0.7076740264892578\n",
      "Iteration 1600, loss=0.7150500416755676\n",
      "Iteration 1700, loss=0.6910930871963501\n",
      "Iteration 1800, loss=0.6838639378547668\n",
      "Iteration 1900, loss=0.7235263586044312\n",
      "Iteration 2000, loss=0.705822229385376\n",
      "Iteration 2100, loss=0.7034331560134888\n",
      "Iteration 2200, loss=0.6816883087158203\n",
      "Iteration 2300, loss=0.7239951491355896\n",
      "Iteration 2400, loss=0.720209002494812\n",
      "Iteration 2500, loss=0.7193384766578674\n",
      "Iteration 2600, loss=0.6874085068702698\n",
      "Iteration 2700, loss=0.6764922738075256\n",
      "Iteration 2800, loss=0.714702844619751\n",
      "Iteration 2900, loss=0.7102102637290955\n",
      "Iteration 3000, loss=0.692974865436554\n",
      "Iteration 3100, loss=0.6850268244743347\n",
      "Iteration 3200, loss=0.6985917091369629\n",
      "Iteration 3300, loss=0.6960874795913696\n",
      "Iteration 3400, loss=0.6963911056518555\n",
      "Iteration 3500, loss=0.7083006501197815\n",
      "Iteration 3600, loss=0.6962198615074158\n",
      "Iteration 3700, loss=0.698483943939209\n",
      "Iteration 3800, loss=0.6880136728286743\n",
      "Iteration 3900, loss=0.6971631050109863\n",
      "Iteration 4000, loss=0.7095656991004944\n",
      "Iteration 4100, loss=0.6982640624046326\n",
      "Iteration 4200, loss=0.7083961963653564\n",
      "Iteration 4300, loss=0.6986560821533203\n",
      "Iteration 4400, loss=0.7092896699905396\n",
      "Iteration 4500, loss=0.6863991618156433\n",
      "Iteration 4600, loss=0.7127194404602051\n",
      "Iteration 4700, loss=0.7263611555099487\n",
      "Iteration 4800, loss=0.707030177116394\n",
      "Iteration 4900, loss=0.7164268493652344\n",
      "Iteration 5000, loss=0.6937581300735474\n",
      "Iteration 5100, loss=0.6942709684371948\n",
      "Iteration 5200, loss=0.6769332885742188\n",
      "Iteration 5300, loss=0.6972202658653259\n",
      "Iteration 5400, loss=0.6663137078285217\n",
      "Iteration 5500, loss=0.6929501295089722\n",
      "Iteration 5600, loss=0.7011017203330994\n",
      "Iteration 5700, loss=0.7226074934005737\n",
      "Iteration 5800, loss=0.6616690158843994\n",
      "Iteration 5900, loss=0.7137584686279297\n",
      "Iteration 6000, loss=0.6779661774635315\n",
      "Iteration 6100, loss=0.7265752553939819\n",
      "Iteration 6200, loss=0.6672790050506592\n",
      "Iteration 6300, loss=0.7120504379272461\n",
      "Iteration 6400, loss=0.7170996069908142\n",
      "Iteration 6500, loss=0.6907374262809753\n",
      "Iteration 6600, loss=0.6867117881774902\n",
      "Iteration 6700, loss=0.6992658972740173\n",
      "Iteration 6800, loss=0.6839646100997925\n",
      "Iteration 6900, loss=0.709212601184845\n",
      "Iteration 7000, loss=0.6765494346618652\n",
      "Iteration 7100, loss=0.6879622936248779\n",
      "Iteration 7200, loss=0.7211871147155762\n",
      "Iteration 7300, loss=0.6817876696586609\n",
      "Iteration 7400, loss=0.6843355298042297\n",
      "Iteration 7500, loss=0.6862570643424988\n",
      "Iteration 7600, loss=0.6997167468070984\n",
      "Iteration 7700, loss=0.704975426197052\n",
      "Iteration 7800, loss=0.6746223568916321\n",
      "Iteration 7900, loss=0.6756858825683594\n",
      "Iteration 8000, loss=0.681473433971405\n",
      "Iteration 8100, loss=0.6965064406394958\n",
      "Iteration 8200, loss=0.681022047996521\n",
      "Iteration 8300, loss=0.7019250392913818\n",
      "Iteration 8400, loss=0.7059277892112732\n",
      "Iteration 8500, loss=0.6732947826385498\n",
      "Iteration 8600, loss=0.6703732013702393\n",
      "Iteration 8700, loss=0.6925051212310791\n",
      "Iteration 8800, loss=0.7031553983688354\n",
      "Iteration 8900, loss=0.6785763502120972\n",
      "Iteration 9000, loss=0.699582040309906\n",
      "Iteration 9100, loss=0.6786782145500183\n",
      "Iteration 9200, loss=0.6911375522613525\n",
      "Iteration 9300, loss=0.7103266716003418\n",
      "Iteration 9400, loss=0.6901792883872986\n",
      "Iteration 9500, loss=0.7104655504226685\n",
      "Iteration 9600, loss=0.6967107057571411\n",
      "Iteration 9700, loss=0.6958799958229065\n",
      "Iteration 9800, loss=0.7049236297607422\n",
      "Iteration 9900, loss=0.6920784711837769\n",
      "Iteration 10000, loss=0.7094188332557678\n",
      "Nearest to over: requires, significance, ill, transmitted, sultan, see, curve, irregular,\n",
      "Nearest to zero: one, but, two, m, width, nine, as, frank,\n",
      "Nearest to six: one, nine, games, eight, the, download, reduced, season,\n",
      "Nearest to would: while, ultimately, sink, sanctuary, enforced, ears, chosen, transmit,\n",
      "Nearest to to: contributions, centred, put, head, referendum, used, invested, opening,\n",
      "Nearest to than: median, far, genesis, predominant, watson, arabic, civilizations, guerrilla,\n",
      "Nearest to on: graphite, situated, brands, performed, died, models, say, amplitude,\n",
      "Nearest to the: conversion, be, virgin, judaism, language, actively, up, majority,\n",
      "Nearest to so: arise, canterbury, consonants, career, employee, release, behavioral, lt,\n",
      "Nearest to more: discovery, placed, shall, eu, president, longer, landscape, india,\n",
      "Nearest to states: conscious, black, generic, crash, drinking, barnes, open, thinking,\n",
      "Nearest to its: knights, chlorine, enemies, humanism, counterpart, thou, adds, levels,\n",
      "Nearest to only: white, after, fully, hate, amiga, eve, times, sing,\n",
      "Nearest to in: area, production, temples, atoms, technology, bicycle, doom, childhood,\n",
      "Nearest to were: started, periodic, opinions, eclipse, nor, placing, oxidation, agents,\n",
      "Nearest to his: kai, international, teacher, daughter, come, work, older, progress,\n",
      "Iteration 10100, loss=0.7002148628234863\n",
      "Iteration 10200, loss=0.7231388092041016\n",
      "Iteration 10300, loss=0.7004948854446411\n",
      "Iteration 10400, loss=0.6874486804008484\n",
      "Iteration 10500, loss=0.7018334865570068\n",
      "Iteration 10600, loss=0.6899104714393616\n",
      "Iteration 10700, loss=0.6946224570274353\n",
      "Iteration 10800, loss=0.6958199739456177\n",
      "Iteration 10900, loss=0.6926049590110779\n",
      "Iteration 11000, loss=0.7063263058662415\n",
      "Iteration 11100, loss=0.6988521218299866\n",
      "Iteration 11200, loss=0.6957813501358032\n",
      "Iteration 11300, loss=0.6836676597595215\n",
      "Iteration 11400, loss=0.6982589364051819\n",
      "Iteration 11500, loss=0.7179361581802368\n",
      "Iteration 11600, loss=0.6939277052879333\n",
      "Iteration 11700, loss=0.7018422484397888\n",
      "Iteration 11800, loss=0.703512966632843\n",
      "Iteration 11900, loss=0.6652967929840088\n",
      "Iteration 12000, loss=0.6956730484962463\n",
      "Iteration 12100, loss=0.7150400876998901\n",
      "Iteration 12200, loss=0.7153308391571045\n",
      "Iteration 12300, loss=0.720020592212677\n",
      "Iteration 12400, loss=0.6769356727600098\n",
      "Iteration 12500, loss=0.7234126925468445\n",
      "Iteration 12600, loss=0.7232426404953003\n",
      "Iteration 12700, loss=0.7039521932601929\n",
      "Iteration 12800, loss=0.7088208794593811\n",
      "Iteration 12900, loss=0.6659666895866394\n",
      "Iteration 13000, loss=0.6835406422615051\n",
      "Iteration 13100, loss=0.6918054223060608\n",
      "Iteration 13200, loss=0.6873897314071655\n",
      "Iteration 13300, loss=0.7056278586387634\n",
      "Iteration 13400, loss=0.6742783784866333\n",
      "Iteration 13500, loss=0.6958920955657959\n",
      "Iteration 13600, loss=0.7050284743309021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13700, loss=0.6868282556533813\n",
      "Iteration 13800, loss=0.7031811475753784\n",
      "Iteration 13900, loss=0.7020870447158813\n",
      "Iteration 14000, loss=0.7105399966239929\n",
      "Iteration 14100, loss=0.694953441619873\n",
      "Iteration 14200, loss=0.6881732940673828\n",
      "Iteration 14300, loss=0.6862485408782959\n",
      "Iteration 14400, loss=0.6981500387191772\n",
      "Iteration 14500, loss=0.686954140663147\n",
      "Iteration 14600, loss=0.7026594877243042\n",
      "Iteration 14700, loss=0.7102077603340149\n",
      "Iteration 14800, loss=0.6885406374931335\n",
      "Iteration 14900, loss=0.6868906617164612\n",
      "Iteration 15000, loss=0.6880958676338196\n",
      "Iteration 15100, loss=0.6919358372688293\n",
      "Iteration 15200, loss=0.7240684032440186\n",
      "Iteration 15300, loss=0.7001110315322876\n",
      "Iteration 15400, loss=0.6927030086517334\n",
      "Iteration 15500, loss=0.7109410762786865\n",
      "Iteration 15600, loss=0.7077505588531494\n",
      "Iteration 15700, loss=0.6788397431373596\n",
      "Iteration 15800, loss=0.7084572911262512\n",
      "Iteration 15900, loss=0.742889940738678\n",
      "Iteration 16000, loss=0.714372456073761\n",
      "Iteration 16100, loss=0.6677057147026062\n",
      "Iteration 16200, loss=0.6876450777053833\n",
      "Iteration 16300, loss=0.7318906188011169\n",
      "Iteration 16400, loss=0.7310175895690918\n",
      "Iteration 16500, loss=0.7153525352478027\n",
      "Iteration 16600, loss=0.6539145708084106\n",
      "Iteration 16700, loss=0.7115914225578308\n",
      "Iteration 16800, loss=0.6671784520149231\n",
      "Iteration 16900, loss=0.71164870262146\n",
      "Iteration 17000, loss=0.7106590867042542\n",
      "Iteration 17100, loss=0.7057281136512756\n",
      "Iteration 17200, loss=0.6824347972869873\n",
      "Iteration 17300, loss=0.719829261302948\n",
      "Iteration 17400, loss=0.7070938348770142\n",
      "Iteration 17500, loss=0.7072793245315552\n",
      "Iteration 17600, loss=0.6959221363067627\n",
      "Iteration 17700, loss=0.6859596967697144\n",
      "Iteration 17800, loss=0.6841010451316833\n",
      "Iteration 17900, loss=0.6747159361839294\n",
      "Iteration 18000, loss=0.7124536633491516\n",
      "Iteration 18100, loss=0.7132790684700012\n",
      "Iteration 18200, loss=0.7076488733291626\n",
      "Iteration 18300, loss=0.6786057949066162\n",
      "Iteration 18400, loss=0.6952755451202393\n",
      "Iteration 18500, loss=0.6546201109886169\n",
      "Iteration 18600, loss=0.7015201449394226\n",
      "Iteration 18700, loss=0.6894006729125977\n",
      "Iteration 18800, loss=0.6811733245849609\n",
      "Iteration 18900, loss=0.6867843866348267\n",
      "Iteration 19000, loss=0.6831294298171997\n",
      "Iteration 19100, loss=0.6971175074577332\n",
      "Iteration 19200, loss=0.5964096188545227\n",
      "Iteration 19300, loss=0.7050265669822693\n",
      "Iteration 19400, loss=0.7031782269477844\n",
      "Iteration 19500, loss=0.7014422416687012\n",
      "Iteration 19600, loss=0.691704511642456\n",
      "Iteration 19700, loss=0.6850107312202454\n",
      "Iteration 19800, loss=0.6738907694816589\n",
      "Iteration 19900, loss=0.7066271901130676\n",
      "Iteration 20000, loss=0.7149910926818848\n",
      "Nearest to over: cases, in, staying, ill, requires, sultan, red, aphrodite,\n",
      "Nearest to zero: four, one, two, nine, eight, mph, t, but,\n",
      "Nearest to six: one, eight, nine, seven, four, five, games, z,\n",
      "Nearest to would: party, while, ultimately, curve, sink, ears, controversial, licensing,\n",
      "Nearest to to: head, with, used, credits, who, appointed, produce, centred,\n",
      "Nearest to than: more, sleep, five, longer, be, median, genesis, civilizations,\n",
      "Nearest to on: and, indeed, guitar, brands, performed, philosophical, say, composite,\n",
      "Nearest to the: of, four, current, majority, remaining, this, and, animated,\n",
      "Nearest to so: free, tension, arise, doing, lt, discovering, fl, career,\n",
      "Nearest to more: than, finish, longer, farther, gregory, eu, shall, placed,\n",
      "Nearest to states: participated, black, crash, conscious, wife, generic, thinking, ammonia,\n",
      "Nearest to its: knights, enemies, characterized, humanism, chlorine, ddt, thou, mainland,\n",
      "Nearest to only: white, axioms, after, times, sing, aggression, amiga, fully,\n",
      "Nearest to in: manner, one, iii, three, a, russia, performing, the,\n",
      "Nearest to were: removed, clergy, started, powerful, holdings, apparently, service, opinions,\n",
      "Nearest to his: of, kai, garfield, protagonist, bonaparte, translate, international, lover,\n",
      "Iteration 20100, loss=0.6774306893348694\n",
      "Iteration 20200, loss=0.7022049427032471\n",
      "Iteration 20300, loss=0.6846097111701965\n",
      "Iteration 20400, loss=0.6843485236167908\n",
      "Iteration 20500, loss=0.6963345408439636\n",
      "Iteration 20600, loss=0.6594240665435791\n",
      "Iteration 20700, loss=0.6524877548217773\n",
      "Iteration 20800, loss=0.6948275566101074\n",
      "Iteration 20900, loss=0.6867157816886902\n",
      "Iteration 21000, loss=0.711699366569519\n",
      "Iteration 21100, loss=0.7004241943359375\n",
      "Iteration 21200, loss=0.7035287022590637\n",
      "Iteration 21300, loss=0.7195178270339966\n",
      "Iteration 21400, loss=0.6759424805641174\n",
      "Iteration 21500, loss=0.667692244052887\n",
      "Iteration 21600, loss=0.7200731039047241\n",
      "Iteration 21700, loss=0.6980225443840027\n",
      "Iteration 21800, loss=0.7156820297241211\n",
      "Iteration 21900, loss=0.7049793004989624\n",
      "Iteration 22000, loss=0.6802932620048523\n",
      "Iteration 22100, loss=0.6561251282691956\n",
      "Iteration 22200, loss=0.6961330771446228\n",
      "Iteration 22300, loss=0.720571756362915\n",
      "Iteration 22400, loss=0.6671385765075684\n",
      "Iteration 22500, loss=0.6909703016281128\n",
      "Iteration 22600, loss=0.6567472219467163\n",
      "Iteration 22700, loss=0.6993169188499451\n",
      "Iteration 22800, loss=0.6991674304008484\n",
      "Iteration 22900, loss=0.6629214882850647\n",
      "Iteration 23000, loss=0.695331871509552\n",
      "Iteration 23100, loss=0.714269757270813\n",
      "Iteration 23200, loss=0.6492846012115479\n",
      "Iteration 23300, loss=0.7269088625907898\n",
      "Iteration 23400, loss=0.6622963547706604\n",
      "Iteration 23500, loss=0.7197180986404419\n",
      "Iteration 23600, loss=0.7066124677658081\n",
      "Iteration 23700, loss=0.70097815990448\n",
      "Iteration 23800, loss=0.6742441058158875\n",
      "Iteration 23900, loss=0.7126521468162537\n",
      "Iteration 24000, loss=0.6910448670387268\n",
      "Iteration 24100, loss=0.6820030808448792\n",
      "Iteration 24200, loss=0.6915408372879028\n",
      "Iteration 24300, loss=0.6755247116088867\n",
      "Iteration 24400, loss=0.6839671730995178\n",
      "Iteration 24500, loss=0.6854645609855652\n",
      "Iteration 24600, loss=0.674457311630249\n",
      "Iteration 24700, loss=0.7038900852203369\n",
      "Iteration 24800, loss=0.7122145295143127\n",
      "Iteration 24900, loss=0.713298499584198\n",
      "Iteration 25000, loss=0.649761974811554\n",
      "Iteration 25100, loss=0.6335182189941406\n",
      "Iteration 25200, loss=0.678187906742096\n",
      "Iteration 25300, loss=0.7162790298461914\n",
      "Iteration 25400, loss=0.7603600025177002\n",
      "Iteration 25500, loss=0.7077513933181763\n",
      "Iteration 25600, loss=0.710686445236206\n",
      "Iteration 25700, loss=0.6838022470474243\n",
      "Iteration 25800, loss=0.6924914717674255\n",
      "Iteration 25900, loss=0.7081108689308167\n",
      "Iteration 26000, loss=0.6763907074928284\n",
      "Iteration 26100, loss=0.6946835517883301\n",
      "Iteration 26200, loss=0.6895187497138977\n",
      "Iteration 26300, loss=0.6565396785736084\n",
      "Iteration 26400, loss=0.6537458300590515\n",
      "Iteration 26500, loss=0.7361629605293274\n",
      "Iteration 26600, loss=0.668253481388092\n",
      "Iteration 26700, loss=0.6847087144851685\n",
      "Iteration 26800, loss=0.6924433708190918\n",
      "Iteration 26900, loss=0.7372941374778748\n",
      "Iteration 27000, loss=0.7078481912612915\n",
      "Iteration 27100, loss=0.6650082468986511\n",
      "Iteration 27200, loss=0.6621928215026855\n",
      "Iteration 27300, loss=0.6878791451454163\n",
      "Iteration 27400, loss=0.7093172669410706\n",
      "Iteration 27500, loss=0.6435502767562866\n",
      "Iteration 27600, loss=0.7254517078399658\n",
      "Iteration 27700, loss=0.711469829082489\n",
      "Iteration 27800, loss=0.72299724817276\n",
      "Iteration 27900, loss=0.7212424278259277\n",
      "Iteration 28000, loss=0.6774551272392273\n",
      "Iteration 28100, loss=0.7416938543319702\n",
      "Iteration 28200, loss=0.7179471850395203\n",
      "Iteration 28300, loss=0.7140392065048218\n",
      "Iteration 28400, loss=0.672851026058197\n",
      "Iteration 28500, loss=0.6626867651939392\n",
      "Iteration 28600, loss=0.6215839385986328\n",
      "Iteration 28700, loss=0.7306475043296814\n",
      "Iteration 28800, loss=0.693677544593811\n",
      "Iteration 28900, loss=0.6898484826087952\n",
      "Iteration 29000, loss=0.7072243094444275\n",
      "Iteration 29100, loss=0.688862681388855\n",
      "Iteration 29200, loss=0.638375461101532\n",
      "Iteration 29300, loss=0.6932975053787231\n",
      "Iteration 29400, loss=0.6990257501602173\n",
      "Iteration 29500, loss=0.6778000593185425\n",
      "Iteration 29600, loss=0.6690417528152466\n",
      "Iteration 29700, loss=0.6491502523422241\n",
      "Iteration 29800, loss=0.6677785515785217\n",
      "Iteration 29900, loss=0.7309792041778564\n",
      "Iteration 30000, loss=0.7096739411354065\n",
      "Nearest to over: cases, in, the, vietnamese, red, staying, comments, requires,\n",
      "Nearest to zero: one, four, nine, eight, two, six, m, mph,\n",
      "Nearest to six: one, eight, nine, seven, four, five, three, zero,\n",
      "Nearest to would: ultimately, party, licensing, curve, sanctuary, controversial, sink, enforced,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to to: with, the, of, head, a, in, be, and,\n",
      "Nearest to than: more, sleep, longer, weapon, five, be, civilizations, across,\n",
      "Nearest to on: surgery, brands, transmitted, guitar, philosophical, indeed, sort, totally,\n",
      "Nearest to the: of, in, four, one, to, and, nine, seven,\n",
      "Nearest to so: refined, free, tension, inserted, doing, min, market, fl,\n",
      "Nearest to more: than, finish, longer, gregory, training, eu, farther, shall,\n",
      "Nearest to states: black, crash, participated, from, conscious, ammonia, wife, franco,\n",
      "Nearest to its: knights, enemies, characterized, humanism, territories, because, mainland, screen,\n",
      "Nearest to only: white, after, axioms, times, fully, design, protocol, eve,\n",
      "Nearest to in: the, one, russia, but, a, of, rail, seven,\n",
      "Nearest to were: removed, service, fire, rated, surroundings, agents, placing, clergy,\n",
      "Nearest to his: of, takes, protagonist, kai, he, the, tribes, foster,\n",
      "Iteration 30100, loss=0.6883077621459961\n",
      "Iteration 30200, loss=0.7079841494560242\n",
      "Iteration 30300, loss=0.7179646492004395\n",
      "Iteration 30400, loss=0.6785838603973389\n",
      "Iteration 30500, loss=0.6702213883399963\n",
      "Iteration 30600, loss=0.6878990530967712\n",
      "Iteration 30700, loss=0.6571937203407288\n",
      "Iteration 30800, loss=0.7340186834335327\n",
      "Iteration 30900, loss=0.6999754309654236\n",
      "Iteration 31000, loss=0.6865780353546143\n",
      "Iteration 31100, loss=0.6896784901618958\n",
      "Iteration 31200, loss=0.5692581534385681\n",
      "Iteration 31300, loss=0.6677792072296143\n",
      "Iteration 31400, loss=0.6749476194381714\n",
      "Iteration 31500, loss=0.6813268661499023\n",
      "Iteration 31600, loss=0.7171971201896667\n",
      "Iteration 31700, loss=0.6956557631492615\n",
      "Iteration 31800, loss=0.7230587601661682\n",
      "Iteration 31900, loss=0.6854078769683838\n",
      "Iteration 32000, loss=0.697309672832489\n",
      "Iteration 32100, loss=0.6133450865745544\n",
      "Iteration 32200, loss=0.6911438703536987\n",
      "Iteration 32300, loss=0.6770039796829224\n",
      "Iteration 32400, loss=0.6611818671226501\n",
      "Iteration 32500, loss=0.7106858491897583\n",
      "Iteration 32600, loss=0.6459990739822388\n",
      "Iteration 32700, loss=0.743377685546875\n",
      "Iteration 32800, loss=0.696363627910614\n",
      "Iteration 32900, loss=0.7457290291786194\n",
      "Iteration 33000, loss=0.7058549523353577\n",
      "Iteration 33100, loss=0.6701980829238892\n",
      "Iteration 33200, loss=0.6933910846710205\n",
      "Iteration 33300, loss=0.7416800856590271\n",
      "Iteration 33400, loss=0.7194539904594421\n",
      "Iteration 33500, loss=0.7114023566246033\n",
      "Iteration 33600, loss=0.6886448264122009\n",
      "Iteration 33700, loss=0.6906071901321411\n",
      "Iteration 33800, loss=0.7556240558624268\n",
      "Iteration 33900, loss=0.7072025537490845\n",
      "Iteration 34000, loss=0.6977336406707764\n",
      "Iteration 34100, loss=0.5895258784294128\n",
      "Iteration 34200, loss=0.7722888588905334\n",
      "Iteration 34300, loss=0.6487065553665161\n",
      "Iteration 34400, loss=0.7487912178039551\n",
      "Iteration 34500, loss=0.6243100166320801\n",
      "Iteration 34600, loss=0.6458628177642822\n",
      "Iteration 34700, loss=0.6923414468765259\n",
      "Iteration 34800, loss=0.6842812299728394\n",
      "Iteration 34900, loss=0.6552785634994507\n",
      "Iteration 35000, loss=0.7280445098876953\n",
      "Iteration 35100, loss=0.7038567662239075\n",
      "Iteration 35200, loss=0.7784181833267212\n",
      "Iteration 35300, loss=0.6918346285820007\n",
      "Iteration 35400, loss=0.6947625875473022\n",
      "Iteration 35500, loss=0.6687718033790588\n",
      "Iteration 35600, loss=0.6888617277145386\n",
      "Iteration 35700, loss=0.7094966173171997\n",
      "Iteration 35800, loss=0.7144675254821777\n",
      "Iteration 35900, loss=0.6957148313522339\n",
      "Iteration 36000, loss=0.6964524388313293\n",
      "Iteration 36100, loss=0.507520318031311\n",
      "Iteration 36200, loss=0.7034894227981567\n",
      "Iteration 36300, loss=0.7143791317939758\n",
      "Iteration 36400, loss=0.6842745542526245\n",
      "Iteration 36500, loss=0.6986509561538696\n",
      "Iteration 36600, loss=0.606412947177887\n",
      "Iteration 36700, loss=0.7209718227386475\n",
      "Iteration 36800, loss=0.6984800100326538\n",
      "Iteration 36900, loss=0.7062446475028992\n",
      "Iteration 37000, loss=0.620226263999939\n",
      "Iteration 37100, loss=0.7206754684448242\n",
      "Iteration 37200, loss=0.698941171169281\n",
      "Iteration 37300, loss=0.7110040783882141\n",
      "Iteration 37400, loss=0.6735355854034424\n",
      "Iteration 37500, loss=0.6271977424621582\n",
      "Iteration 37600, loss=0.6930707693099976\n",
      "Iteration 37700, loss=0.6820766925811768\n",
      "Iteration 37800, loss=0.7782390117645264\n",
      "Iteration 37900, loss=0.7596624493598938\n",
      "Iteration 38000, loss=0.7191655039787292\n",
      "Iteration 38100, loss=0.6600797772407532\n",
      "Iteration 38200, loss=0.5479415655136108\n",
      "Iteration 38300, loss=0.6918782591819763\n",
      "Iteration 38400, loss=0.7088577151298523\n",
      "Iteration 38500, loss=0.7322304844856262\n",
      "Iteration 38600, loss=0.7050509452819824\n",
      "Iteration 38700, loss=0.6885594725608826\n",
      "Iteration 38800, loss=0.7099334001541138\n",
      "Iteration 38900, loss=0.5582273602485657\n",
      "Iteration 39000, loss=0.6987292766571045\n",
      "Iteration 39100, loss=0.7097094058990479\n",
      "Iteration 39200, loss=0.6990259289741516\n",
      "Iteration 39300, loss=0.546186089515686\n",
      "Iteration 39400, loss=0.7317047715187073\n",
      "Iteration 39500, loss=0.5957893133163452\n",
      "Iteration 39600, loss=0.718603789806366\n",
      "Iteration 39700, loss=0.6765813231468201\n",
      "Iteration 39800, loss=0.6444762945175171\n",
      "Iteration 39900, loss=0.6457614302635193\n",
      "Iteration 40000, loss=0.7726861834526062\n",
      "Nearest to over: the, of, in, city, with, and, cases, a,\n",
      "Nearest to zero: one, four, two, nine, eight, six, five, three,\n",
      "Nearest to six: one, nine, five, seven, three, eight, zero, four,\n",
      "Nearest to would: party, abandon, licensing, sanctuary, while, curve, case, sink,\n",
      "Nearest to to: the, of, in, a, with, and, for, be,\n",
      "Nearest to than: more, sleep, with, is, a, from, longer, weapon,\n",
      "Nearest to on: philosophical, iron, and, surgery, indeed, may, spent, sort,\n",
      "Nearest to the: of, in, is, and, to, one, a, four,\n",
      "Nearest to so: refined, tension, be, min, inserted, free, second, fl,\n",
      "Nearest to more: than, longer, finish, properties, unofficial, stable, gregory, deemed,\n",
      "Nearest to states: black, crash, participated, germany, conscious, from, ammonia, wife,\n",
      "Nearest to its: knights, characterized, enemies, console, territories, slaves, chlorine, levels,\n",
      "Nearest to only: white, to, the, with, a, of, six, one,\n",
      "Nearest to in: the, one, six, to, a, three, five, of,\n",
      "Nearest to were: store, susceptible, productive, removed, greece, three, england, get,\n",
      "Nearest to his: of, the, rank, tribes, a, he, kai, until,\n",
      "Iteration 40100, loss=0.6305890679359436\n",
      "Iteration 40200, loss=0.7612651586532593\n",
      "Iteration 40300, loss=0.6459345817565918\n",
      "Iteration 40400, loss=0.6871868371963501\n",
      "Iteration 40500, loss=0.7430064082145691\n",
      "Iteration 40600, loss=0.718798041343689\n",
      "Iteration 40700, loss=0.5559506416320801\n",
      "Iteration 40800, loss=0.8500193357467651\n",
      "Iteration 40900, loss=0.6584632992744446\n",
      "Iteration 41000, loss=0.7157137393951416\n",
      "Iteration 41100, loss=0.6942213773727417\n",
      "Iteration 41200, loss=0.9859229922294617\n",
      "Iteration 41300, loss=0.663102924823761\n",
      "Iteration 41400, loss=0.6599271297454834\n",
      "Iteration 41500, loss=0.6248480677604675\n",
      "Iteration 41600, loss=0.7444359660148621\n",
      "Iteration 41700, loss=0.6180165410041809\n",
      "Iteration 41800, loss=0.754075825214386\n",
      "Iteration 41900, loss=0.7182550430297852\n",
      "Iteration 42000, loss=0.6521356701850891\n",
      "Iteration 42100, loss=0.6639167070388794\n",
      "Iteration 42200, loss=0.607438862323761\n",
      "Iteration 42300, loss=0.6925265192985535\n",
      "Iteration 42400, loss=0.6472710967063904\n",
      "Iteration 42500, loss=0.6436101794242859\n",
      "Iteration 42600, loss=4.470449130167253e-05\n",
      "Iteration 42700, loss=0.7069103121757507\n",
      "Iteration 42800, loss=0.6501103043556213\n",
      "Iteration 42900, loss=0.606050968170166\n",
      "Iteration 43000, loss=0.7345329523086548\n",
      "Iteration 43100, loss=0.6659592390060425\n",
      "Iteration 43200, loss=0.7047669291496277\n",
      "Iteration 43300, loss=0.7228797674179077\n",
      "Iteration 43400, loss=0.6718054413795471\n",
      "Iteration 43500, loss=0.6561857461929321\n",
      "Iteration 43600, loss=0.6627611517906189\n",
      "Iteration 43700, loss=0.6941830515861511\n",
      "Iteration 43800, loss=0.630392849445343\n",
      "Iteration 43900, loss=0.7000631093978882\n",
      "Iteration 44000, loss=1.1072276830673218\n",
      "Iteration 44100, loss=0.6441908478736877\n",
      "Iteration 44200, loss=0.6891233921051025\n",
      "Iteration 44300, loss=0.6934454441070557\n",
      "Iteration 44400, loss=0.7453340291976929\n",
      "Iteration 44500, loss=0.6526526212692261\n",
      "Iteration 44600, loss=0.6286250352859497\n",
      "Iteration 44700, loss=0.649106502532959\n",
      "Iteration 44800, loss=0.6455739736557007\n",
      "Iteration 44900, loss=0.6644396781921387\n",
      "Iteration 45000, loss=0.7192730903625488\n",
      "Iteration 45100, loss=0.7604873180389404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45200, loss=0.7020692825317383\n",
      "Iteration 45300, loss=0.6833928227424622\n",
      "Iteration 45400, loss=0.6811521053314209\n",
      "Iteration 45500, loss=0.6023934483528137\n",
      "Iteration 45600, loss=0.6793574690818787\n",
      "Iteration 45700, loss=0.7900888323783875\n",
      "Iteration 45800, loss=0.6824355721473694\n",
      "Iteration 45900, loss=0.6988028883934021\n",
      "Iteration 46000, loss=0.6467723250389099\n",
      "Iteration 46100, loss=0.6169424057006836\n",
      "Iteration 46200, loss=0.6276608109474182\n",
      "Iteration 46300, loss=0.27480804920196533\n",
      "Iteration 46400, loss=0.7071689367294312\n",
      "Iteration 46500, loss=0.6608577370643616\n",
      "Iteration 46600, loss=0.6563248038291931\n",
      "Iteration 46700, loss=0.7581914067268372\n",
      "Iteration 46800, loss=0.8004084229469299\n",
      "Iteration 46900, loss=0.6578397750854492\n",
      "Iteration 47000, loss=0.5806893706321716\n",
      "Iteration 47100, loss=0.7536967396736145\n",
      "Iteration 47200, loss=0.7847437262535095\n",
      "Iteration 47300, loss=0.733802318572998\n",
      "Iteration 47400, loss=0.8913204073905945\n",
      "Iteration 47500, loss=0.6621750593185425\n",
      "Iteration 47600, loss=0.641063928604126\n",
      "Iteration 47700, loss=0.4725813865661621\n",
      "Iteration 47800, loss=0.7063769102096558\n",
      "Iteration 47900, loss=0.6138138771057129\n",
      "Iteration 48000, loss=0.6232655048370361\n",
      "Iteration 48100, loss=0.5962714552879333\n",
      "Iteration 48200, loss=0.6241054534912109\n",
      "Iteration 48300, loss=0.5510020852088928\n",
      "Iteration 48400, loss=0.6457073092460632\n",
      "Iteration 48500, loss=0.6068027019500732\n",
      "Iteration 48600, loss=0.6004647612571716\n",
      "Iteration 48700, loss=0.5458860993385315\n",
      "Iteration 48800, loss=0.6352491974830627\n",
      "Iteration 48900, loss=0.6867036819458008\n",
      "Iteration 49000, loss=0.6512279510498047\n",
      "Iteration 49100, loss=0.6137397885322571\n",
      "Iteration 49200, loss=0.6325693130493164\n",
      "Iteration 49300, loss=0.7190448045730591\n",
      "Iteration 49400, loss=0.2813539206981659\n",
      "Iteration 49500, loss=0.27260586619377136\n",
      "Iteration 49600, loss=0.6743897199630737\n",
      "Iteration 49700, loss=0.7744973301887512\n",
      "Iteration 49800, loss=0.7849623560905457\n",
      "Iteration 49900, loss=0.6692551970481873\n",
      "Iteration 50000, loss=0.6719755530357361\n",
      "Nearest to over: the, and, in, with, of, city, a, for,\n",
      "Nearest to zero: one, nine, two, five, four, six, eight, three,\n",
      "Nearest to six: one, seven, nine, five, three, two, eight, zero,\n",
      "Nearest to would: assist, all, party, abandon, case, licensing, sanctuary, there,\n",
      "Nearest to to: the, be, a, of, and, with, in, five,\n",
      "Nearest to than: more, five, is, back, of, strategy, a, with,\n",
      "Nearest to on: and, depends, the, in, five, nine, surgery, philosophical,\n",
      "Nearest to the: of, is, and, in, for, to, a, five,\n",
      "Nearest to so: refined, free, min, second, be, growth, inserted, mac,\n",
      "Nearest to more: than, work, properties, finish, files, stable, longer, acts,\n",
      "Nearest to states: from, black, crash, united, germany, participated, conscious, wife,\n",
      "Nearest to its: trial, attack, console, ordered, settlement, tried, enemies, slaves,\n",
      "Nearest to only: to, the, with, a, one, white, of, zimbabwe,\n",
      "Nearest to in: the, five, one, a, six, of, nine, to,\n",
      "Nearest to were: animated, susceptible, store, greece, three, proving, residents, productive,\n",
      "Nearest to his: of, he, the, for, a, and, minds, come,\n",
      "Iteration 50100, loss=0.7496068477630615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4b2055da148b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0marr_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0marr_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {}, loss={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
